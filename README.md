<h1 align="center">
  <img src="media/kinamic_logo.svg" width="100px">
</h1>

# Kinamic Challenge ğŸ“š

This repository contains a web scraping and data analysis project for the [Books to Scrape](https://books.toscrape.com/) website. The main goals are the create the scraper, process and analyze book data using Python.

---

## ğŸ”§ Technical Specifications

### ğŸš€ How to Run This Project

Follow these steps to set up and run the Python application.

#### ğŸ“ Prerequisites

- Python 3.8.10+ installed
- `pip` installed
- `venv` for virtual environments (Recommended)

#### 1. Clone the Repository

```bash
git clone https://github.com/juancruzromero/kinamic_challenge.git
cd kinamic-challenge
```

#### 2. Create and Activate a Virtual Environment (Recommended)

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```

#### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

#### 4. Run the Scraper

```bash
python main.py
```

Data will be saved in the `data/raw/` and `data/processed/` folders.

---

## ğŸ““ How to Use the Notebook

A data analysis report is provided as a Jupyter Notebook.

To run it (in the virtual environment):

```bash
jupyter notebook
```

Then, open the link shown in the terminal (e.g., `http://localhost:8888/...`) and click on **notebook.ipynb**.

The notebook is also available in `.html` and `.md` formats.

---

## ğŸ¤” Why `requests` + `BeautifulSoup` Instead of `Scrapy`?

While Scrapy is a powerful and scalable scraping framework, this project uses `requests` and `BeautifulSoup` for the following reasons:

| Criteria             | requests + bs4                   | Scrapy                               |
|----------------------|----------------------------------|--------------------------------------|
| ğŸš€ Setup             | Extremely simple                 | More complex (requires full project) |
| ğŸ¯ Precision         | Great for small/medium scrapers  | Overkill for small tasks             |
| ğŸ§µ Async needed?     | âŒ Not needed here (momentarily) | âœ… Great for high concurrency         |

> For a small-scale, single-domain project like this one, `requests` + `bs4` offers greater simplicity, transparency, and flexibility, especially within data notebooks and pipelines.

---

## ğŸ“‚ Project Structure

This is the project's directory layout. The scrapers are placed inside the scrapers/ folder to keep the code modular and scalable. This structure allows for easy expansion in the futureâ€”for example, by adding new scrapers, ETL pipelines, or integrations.

```text
kinamic_challenge/
â”‚
â”œâ”€â”€ config/                   # Configuration files
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/                     # Raw and processed data
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ etls/                     # ETL process logic
â”‚   â””â”€â”€ books_etl.py
â”‚
â”œâ”€â”€ logs/                     # Scraping logs
â”‚
â”œâ”€â”€ media/                    # Media assets (e.g., logo)
â”‚
â”œâ”€â”€ scrapers/                 # Scraping logic for each project
â”‚   â””â”€â”€ books_scraper.py
â”‚
â”œâ”€â”€ tests/                    # Unit and integration tests
â”‚
â”œâ”€â”€ utils/                    # Helper functions, logger and utilities
â”‚   â”œâ”€â”€ helpers.py
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config.yaml
â”œâ”€â”€ main.py
â”œâ”€â”€ notebook.ipynb
â”œâ”€â”€ notebook.html
â”œâ”€â”€ notebook.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## â³ To-Do

New features to add in a next version

- [ ] Add more fields (images, stock, URLs, etc.)
- [ ] Scrape book descriptions (1 request per book)
- [ ] Add multithreading for faster scraping
- [ ] Schedule scraping and ETL using Apache Airflow
- [ ] Finalize tests
- [ ] Improve *main.py* script with argparse and importlib
- [ ] Deploy with Docker

---

## ğŸ§  Author

- Juan Cruz Romero